#!/bin/python

import argparse
#import openai
import requests
#import json
#import uuid
#import time
import sys
import os
from openai import OpenAIError

class ApiKeyFile:
    def __init__(self):
        self.filename = os.path.expanduser("~/.tgpt/api")
        self.api_key = None
        
    def get_api_key(self):
        if self.api_key is None:
            with open(self.filename, 'r') as f:
                self.api_key = f.read().strip()
        return self.api_key


class ChatGPTClient:
    def __init__(self, api_key, model="gpt-3.5-turbo", endpoint="https://api.openai.com/v1/chat/completions"):
        self.api_key = api_key
        self.endpoint = endpoint
        self.model = model
        self.chat_history = []
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"}
        
    def prompt(self, prompt, chat_log=None, stop=None, max_tokens=100, temperature=0.5, top_p=1):
        """
        Send a prompt to the GPT-3 API to generate a completion.

        Args:
            prompt (str): The prompt to send to the API.
            chat_log (Optional[str]): The previous conversation history in the chat format. 
                                    Use only for chat mode.
            stop (Optional[str or List[str]]): Up to 4 sequences where the API will stop generating further tokens.
            max_tokens (int): The maximum number of tokens to generate in the completion.
            temperature (float): What sampling temperature to use, between 0 and 1.
            top_p (float): An alternative to sampling with temperature, where the model 
                        considers the results of the tokens with top_p probability mass.

        Returns:
            The response from the API, containing the generated text.
        """
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }

        payload = {
            'model': 'gpt-3.5-turbo',
            'prompt': prompt,
            'temperature': temperature,
            'max_tokens': max_tokens,
            'top_p': top_p
        }

        if stop is not None:
            payload['stop'] = stop

        if chat_log is not None:
            payload['messages'] = chat_log

        url = 'https://api.openai.com/v1/chat/completions'
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()

        data = response.json()['choices'][0]['text']
        return data.strip()

        
    def completions(self, prompt, max_tokens=100, temperature=0.5, top_p=1, presence_penalty=0, frequency_penalty=0, stop=None, n=1):
        headers = {"Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"}
        data = {
            "model": self.model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "n": n,
            "presence_penalty": presence_penalty,
            "frequency_penalty": frequency_penalty,
            "stop": stop
        }

        messages = [{"role": "user", "content": prompt}]
        data["messages"] = messages

        url = "https://api.openai.com/v1/chat/completions"
        response = requests.post(url, headers=headers, json=data)
        if response.status_code != 200:
            print(response.content.decode())
            raise ValueError("Failed to generate completions")
        response = response.json()
        choices = response["choices"]
        return choices[0]["message"]["content"].strip()

    def generate_text(self, prompt, max_tokens=200, temperature=0.5, top_p=1, frequency_penalty=0, presence_penalty=0):
        payload = {
            "model": self.model,
            "messages": [],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty
        }

        if self.chat_history:
            history = self.chat_history
            history.append({"role": "user", "content": prompt})
            payload["messages"] = history
        else:
            payload["messages"] = [{"role": "user", "content": prompt}]
        
        
        response = requests.post(self.endpoint, headers=self.headers, json=payload)

        #print(payload)
        #print(response.content.decode())

        response.raise_for_status()

        result = response.json()
        choices = result["choices"]
        text = choices[0]["message"]["content"].strip()
        self.add_to_chat_history(prompt, text)
        return text

    def set_api_key(self, api_key):
        self.api_key = api_key

    def set_model(self, model):
        self.model = model

    def list_models(self):
        url = "https://api.openai.com/v1/models"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        response = requests.get(url, headers=headers)
        return [model["id"] for model in response.json()["data"]]

    def add_to_chat_history(self, prompt, response):
        """Adds a prompt and response message to the chat history."""
        prompt_message = {'role': 'user', 'content': prompt}
        response_message = {'role': 'assistant', 'content': response}
        self.chat_history.append(prompt_message)
        self.chat_history.append(response_message)

    def get_chat_history(self):
        return self.chat_history

class CommandLineInterface:
    def __init__(self, client):
        self.client = client

    def run(self):
        """Starts the command line interface in chat mode."""
        print("Welcome to TerminalGPT!")
        print("Type '/exit or /quit' to end the session.")

        while True:
            user_input = input("\nYou: ")
            if user_input.strip() == "":
                continue
            if user_input.startswith("/"):
                if not self.handle_command(user_input):
                    break
            else:
                response = self.client.generate_text(user_input)
                self.print_response(response)

    def print_response(self, response_text):
        print(f"\ngpt: {response_text}")

    def handle_completion(self, prompt):
        """
        Handle a single GPT-3 completion request.

        :param prompt: The user's prompt.
        :return: The response text from the GPT-3 model.
        """
        if not prompt:
            return ""

        # Generate chat completion
        completion = self.client.completions(prompt, max_tokens=1024, n=1, stop=None)

        return completion

    def handle_command(self, command):
        if command == "/exit" or command == "/quit":
            return self._exit()
        elif command == "/help":
            self._print_help()
            return True
        else:
            print("Invalid command, please use one of the following:")
            return self._print_help()

    def _exit(self):
        print("Goodbye!")
        sys.exit()

    def _print_help(self):
        print("Available commands:")
        print("/exit or /quit: Exit the program")
        print("/help: Show this help message")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("question", nargs="?", help="The question to ask ChatGPT")
    parser.add_argument("-c", "--chat", action="store_true", help="Enter chat mode")
    args = parser.parse_args()

    # Load API key from file
    api_key_file = ApiKeyFile()
    api_key = api_key_file.get_api_key()

    # Create client and command line interface objects
    client = ChatGPTClient(api_key, "gpt-3.5-turbo", "https://api.openai.com/v1/chat/completions")
    cli = CommandLineInterface(client)

    if args.question:
        response = cli.handle_completion(args.question)
        cli.print_response(response)
    elif args.chat:
        cli.run()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()

